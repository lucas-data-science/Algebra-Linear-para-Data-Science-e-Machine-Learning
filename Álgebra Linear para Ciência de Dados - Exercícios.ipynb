{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ya79uxioT32P"
   },
   "source": [
    "# Instalações e importações"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "UmrDP3B9hvWR"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting fiftyone\n",
      "  Downloading fiftyone-1.5.2-py3-none-any.whl.metadata (23 kB)\n",
      "Requirement already satisfied: aiofiles in c:\\users\\lucas.souza\\appdata\\local\\anaconda3\\lib\\site-packages (from fiftyone) (22.1.0)\n",
      "Collecting argcomplete (from fiftyone)\n",
      "  Downloading argcomplete-3.6.2-py3-none-any.whl.metadata (16 kB)\n",
      "Requirement already satisfied: async_lru>=2 in c:\\users\\lucas.souza\\appdata\\local\\anaconda3\\lib\\site-packages (from fiftyone) (2.0.4)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\lucas.souza\\appdata\\local\\anaconda3\\lib\\site-packages (from fiftyone) (4.12.3)\n",
      "Collecting boto3 (from fiftyone)\n",
      "  Downloading boto3-1.38.41-py3-none-any.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: cachetools in c:\\users\\lucas.souza\\appdata\\local\\anaconda3\\lib\\site-packages (from fiftyone) (5.4.0)\n",
      "Collecting dacite<1.8.0,>=1.6.0 (from fiftyone)\n",
      "  Downloading dacite-1.7.0-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: dill in c:\\users\\lucas.souza\\appdata\\local\\anaconda3\\lib\\site-packages (from fiftyone) (0.3.6)\n",
      "Collecting Deprecated (from fiftyone)\n",
      "  Downloading Deprecated-1.2.18-py2.py3-none-any.whl.metadata (5.7 kB)\n",
      "Collecting ftfy (from fiftyone)\n",
      "  Downloading ftfy-6.3.1-py3-none-any.whl.metadata (7.3 kB)\n",
      "Collecting humanize (from fiftyone)\n",
      "  Downloading humanize-4.12.3-py3-none-any.whl.metadata (7.8 kB)\n",
      "Collecting hypercorn>=0.13.2 (from fiftyone)\n",
      "  Downloading hypercorn-0.17.3-py3-none-any.whl.metadata (5.4 kB)\n",
      "Requirement already satisfied: Jinja2>=3 in c:\\users\\lucas.souza\\appdata\\local\\anaconda3\\lib\\site-packages (from fiftyone) (3.1.4)\n",
      "Requirement already satisfied: kaleido!=0.2.1.post1 in c:\\users\\lucas.souza\\appdata\\local\\anaconda3\\lib\\site-packages (from fiftyone) (0.2.1)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\lucas.souza\\appdata\\local\\anaconda3\\lib\\site-packages (from fiftyone) (3.9.2)\n",
      "Collecting mongoengine~=0.29.1 (from fiftyone)\n",
      "  Downloading mongoengine-0.29.1-py3-none-any.whl.metadata (6.7 kB)\n",
      "Collecting motor~=3.6.0 (from fiftyone)\n",
      "  Downloading motor-3.6.1-py3-none-any.whl.metadata (21 kB)\n",
      "Requirement already satisfied: numpy in c:\\users\\lucas.souza\\appdata\\local\\anaconda3\\lib\\site-packages (from fiftyone) (1.26.4)\n",
      "Requirement already satisfied: packaging in c:\\users\\lucas.souza\\appdata\\local\\anaconda3\\lib\\site-packages (from fiftyone) (24.1)\n",
      "Requirement already satisfied: pandas in c:\\users\\lucas.souza\\appdata\\local\\anaconda3\\lib\\site-packages (from fiftyone) (2.3.0)\n",
      "Requirement already satisfied: Pillow>=6.2 in c:\\users\\lucas.souza\\appdata\\local\\anaconda3\\lib\\site-packages (from fiftyone) (10.4.0)\n",
      "Requirement already satisfied: plotly>=4.14 in c:\\users\\lucas.souza\\appdata\\local\\anaconda3\\lib\\site-packages (from fiftyone) (6.0.1)\n",
      "Collecting pprintpp (from fiftyone)\n",
      "  Downloading pprintpp-0.4.0-py2.py3-none-any.whl.metadata (7.9 kB)\n",
      "Requirement already satisfied: psutil in c:\\users\\lucas.souza\\appdata\\local\\anaconda3\\lib\\site-packages (from fiftyone) (5.9.0)\n",
      "Collecting pymongo~=4.9.2 (from fiftyone)\n",
      "  Downloading pymongo-4.9.2-cp311-cp311-win_amd64.whl.metadata (22 kB)\n",
      "Requirement already satisfied: pytz in c:\\users\\lucas.souza\\appdata\\local\\anaconda3\\lib\\site-packages (from fiftyone) (2024.1)\n",
      "Requirement already satisfied: PyYAML in c:\\users\\lucas.souza\\appdata\\local\\anaconda3\\lib\\site-packages (from fiftyone) (6.0.1)\n",
      "Requirement already satisfied: regex in c:\\users\\lucas.souza\\appdata\\local\\anaconda3\\lib\\site-packages (from fiftyone) (2023.10.3)\n",
      "Requirement already satisfied: retrying in c:\\users\\lucas.souza\\appdata\\local\\anaconda3\\lib\\site-packages (from fiftyone) (1.3.4)\n",
      "Requirement already satisfied: rtree in c:\\users\\lucas.souza\\appdata\\local\\anaconda3\\lib\\site-packages (from fiftyone) (1.0.1)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\lucas.souza\\appdata\\local\\anaconda3\\lib\\site-packages (from fiftyone) (1.5.1)\n",
      "Requirement already satisfied: scikit-image in c:\\users\\lucas.souza\\appdata\\local\\anaconda3\\lib\\site-packages (from fiftyone) (0.20.0)\n",
      "Requirement already satisfied: scipy in c:\\users\\lucas.souza\\appdata\\local\\anaconda3\\lib\\site-packages (from fiftyone) (1.14.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\lucas.souza\\appdata\\local\\anaconda3\\lib\\site-packages (from fiftyone) (70.0.0)\n",
      "Collecting sseclient-py<2,>=1.7.2 (from fiftyone)\n",
      "  Downloading sseclient_py-1.8.0-py2.py3-none-any.whl.metadata (2.0 kB)\n",
      "Collecting sse-starlette<1,>=0.10.3 (from fiftyone)\n",
      "  Downloading sse_starlette-0.10.3-py3-none-any.whl.metadata (4.3 kB)\n",
      "Collecting starlette>=0.24.0 (from fiftyone)\n",
      "  Downloading starlette-0.47.1-py3-none-any.whl.metadata (6.2 kB)\n",
      "Collecting strawberry-graphql>=0.262.4 (from fiftyone)\n",
      "  Downloading strawberry_graphql-0.275.2-py3-none-any.whl.metadata (7.4 kB)\n",
      "Requirement already satisfied: tabulate in c:\\users\\lucas.souza\\appdata\\local\\anaconda3\\lib\\site-packages (from fiftyone) (0.9.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\lucas.souza\\appdata\\local\\anaconda3\\lib\\site-packages (from fiftyone) (4.65.0)\n",
      "Collecting xmltodict (from fiftyone)\n",
      "  Downloading xmltodict-0.14.2-py2.py3-none-any.whl.metadata (8.0 kB)\n",
      "Collecting universal-analytics-python3<2,>=1.0.1 (from fiftyone)\n",
      "  Downloading universal_analytics_python3-1.1.1-py3-none-any.whl.metadata (5.5 kB)\n",
      "Collecting pydash (from fiftyone)\n",
      "  Downloading pydash-8.0.5-py3-none-any.whl.metadata (4.5 kB)\n",
      "Collecting fiftyone-brain<0.22,>=0.21.2 (from fiftyone)\n",
      "  Downloading fiftyone_brain-0.21.2-py3-none-any.whl.metadata (4.8 kB)\n",
      "Collecting fiftyone-db<2.0,>=0.4 (from fiftyone)\n",
      "  Downloading fiftyone_db-1.1.7-py3-none-win_amd64.whl.metadata (1.2 kB)\n",
      "Collecting voxel51-eta<0.15,>=0.14.0 (from fiftyone)\n",
      "  Downloading voxel51_eta-0.14.2-py2.py3-none-any.whl.metadata (14 kB)\n",
      "Collecting opencv-python-headless (from fiftyone)\n",
      "  Downloading opencv_python_headless-4.11.0.86-cp37-abi3-win_amd64.whl.metadata (20 kB)\n",
      "Collecting h11 (from hypercorn>=0.13.2->fiftyone)\n",
      "  Downloading h11-0.16.0-py3-none-any.whl.metadata (8.3 kB)\n",
      "Collecting h2>=3.1.0 (from hypercorn>=0.13.2->fiftyone)\n",
      "  Downloading h2-4.2.0-py3-none-any.whl.metadata (5.1 kB)\n",
      "Collecting priority (from hypercorn>=0.13.2->fiftyone)\n",
      "  Downloading priority-2.0.0-py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting wsproto>=0.14.0 (from hypercorn>=0.13.2->fiftyone)\n",
      "  Downloading wsproto-1.2.0-py3-none-any.whl.metadata (5.6 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\lucas.souza\\appdata\\local\\anaconda3\\lib\\site-packages (from Jinja2>=3->fiftyone) (2.1.5)\n",
      "Requirement already satisfied: narwhals>=1.15.1 in c:\\users\\lucas.souza\\appdata\\local\\anaconda3\\lib\\site-packages (from plotly>=4.14->fiftyone) (1.35.0)\n",
      "Collecting dnspython<3.0.0,>=1.16.0 (from pymongo~=4.9.2->fiftyone)\n",
      "  Downloading dnspython-2.7.0-py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting anyio<5,>=3.6.2 (from starlette>=0.24.0->fiftyone)\n",
      "  Downloading anyio-4.9.0-py3-none-any.whl.metadata (4.7 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\lucas.souza\\appdata\\local\\anaconda3\\lib\\site-packages (from starlette>=0.24.0->fiftyone) (4.12.2)\n",
      "Collecting graphql-core<3.4.0,>=3.2.0 (from strawberry-graphql>=0.262.4->fiftyone)\n",
      "  Downloading graphql_core-3.2.6-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: python-dateutil<3.0,>=2.7 in c:\\users\\lucas.souza\\appdata\\local\\anaconda3\\lib\\site-packages (from strawberry-graphql>=0.262.4->fiftyone) (2.9.0.post0)\n",
      "Collecting httpx>=0.10.0 (from universal-analytics-python3<2,>=1.0.1->fiftyone)\n",
      "  Downloading httpx-0.28.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Requirement already satisfied: future in c:\\users\\lucas.souza\\appdata\\local\\anaconda3\\lib\\site-packages (from voxel51-eta<0.15,>=0.14.0->fiftyone) (0.18.3)\n",
      "Requirement already satisfied: glob2 in c:\\users\\lucas.souza\\appdata\\local\\anaconda3\\lib\\site-packages (from voxel51-eta<0.15,>=0.14.0->fiftyone) (0.7)\n",
      "Collecting jsonlines (from voxel51-eta<0.15,>=0.14.0->fiftyone)\n",
      "  Downloading jsonlines-4.0.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting py7zr (from voxel51-eta<0.15,>=0.14.0->fiftyone)\n",
      "  Downloading py7zr-1.0.0-py3-none-any.whl.metadata (17 kB)\n",
      "Collecting rarfile (from voxel51-eta<0.15,>=0.14.0->fiftyone)\n",
      "  Downloading rarfile-4.2-py3-none-any.whl.metadata (4.4 kB)\n",
      "Requirement already satisfied: requests in c:\\users\\lucas.souza\\appdata\\local\\anaconda3\\lib\\site-packages (from voxel51-eta<0.15,>=0.14.0->fiftyone) (2.32.3)\n",
      "Requirement already satisfied: six in c:\\users\\lucas.souza\\appdata\\local\\anaconda3\\lib\\site-packages (from voxel51-eta<0.15,>=0.14.0->fiftyone) (1.16.0)\n",
      "Requirement already satisfied: sortedcontainers in c:\\users\\lucas.souza\\appdata\\local\\anaconda3\\lib\\site-packages (from voxel51-eta<0.15,>=0.14.0->fiftyone) (2.4.0)\n",
      "Collecting tzlocal (from voxel51-eta<0.15,>=0.14.0->fiftyone)\n",
      "  Downloading tzlocal-5.3.1-py3-none-any.whl.metadata (7.6 kB)\n",
      "Requirement already satisfied: urllib3 in c:\\users\\lucas.souza\\appdata\\local\\anaconda3\\lib\\site-packages (from voxel51-eta<0.15,>=0.14.0->fiftyone) (1.26.4)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\lucas.souza\\appdata\\local\\anaconda3\\lib\\site-packages (from beautifulsoup4->fiftyone) (2.5)\n",
      "Collecting botocore<1.39.0,>=1.38.41 (from boto3->fiftyone)\n",
      "  Downloading botocore-1.38.41-py3-none-any.whl.metadata (5.7 kB)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in c:\\users\\lucas.souza\\appdata\\local\\anaconda3\\lib\\site-packages (from boto3->fiftyone) (1.0.1)\n",
      "Collecting s3transfer<0.14.0,>=0.13.0 (from boto3->fiftyone)\n",
      "  Downloading s3transfer-0.13.0-py3-none-any.whl.metadata (1.7 kB)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in c:\\users\\lucas.souza\\appdata\\local\\anaconda3\\lib\\site-packages (from Deprecated->fiftyone) (1.14.1)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\lucas.souza\\appdata\\local\\anaconda3\\lib\\site-packages (from ftfy->fiftyone) (0.2.5)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\lucas.souza\\appdata\\local\\anaconda3\\lib\\site-packages (from matplotlib->fiftyone) (1.2.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\lucas.souza\\appdata\\local\\anaconda3\\lib\\site-packages (from matplotlib->fiftyone) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\lucas.souza\\appdata\\local\\anaconda3\\lib\\site-packages (from matplotlib->fiftyone) (4.53.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\lucas.souza\\appdata\\local\\anaconda3\\lib\\site-packages (from matplotlib->fiftyone) (1.4.5)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\lucas.souza\\appdata\\local\\anaconda3\\lib\\site-packages (from matplotlib->fiftyone) (3.1.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\lucas.souza\\appdata\\local\\anaconda3\\lib\\site-packages (from pandas->fiftyone) (2024.1)\n",
      "Requirement already satisfied: networkx>=2.8 in c:\\users\\lucas.souza\\appdata\\local\\anaconda3\\lib\\site-packages (from scikit-image->fiftyone) (3.1)\n",
      "Requirement already satisfied: imageio>=2.4.1 in c:\\users\\lucas.souza\\appdata\\local\\anaconda3\\lib\\site-packages (from scikit-image->fiftyone) (2.31.4)\n",
      "Requirement already satisfied: tifffile>=2019.7.26 in c:\\users\\lucas.souza\\appdata\\local\\anaconda3\\lib\\site-packages (from scikit-image->fiftyone) (2023.4.12)\n",
      "Requirement already satisfied: PyWavelets>=1.1.1 in c:\\users\\lucas.souza\\appdata\\local\\anaconda3\\lib\\site-packages (from scikit-image->fiftyone) (1.4.1)\n",
      "Requirement already satisfied: lazy_loader>=0.1 in c:\\users\\lucas.souza\\appdata\\local\\anaconda3\\lib\\site-packages (from scikit-image->fiftyone) (0.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\lucas.souza\\appdata\\local\\anaconda3\\lib\\site-packages (from scikit-learn->fiftyone) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\lucas.souza\\appdata\\local\\anaconda3\\lib\\site-packages (from scikit-learn->fiftyone) (3.5.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\lucas.souza\\appdata\\local\\anaconda3\\lib\\site-packages (from tqdm->fiftyone) (0.4.6)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\lucas.souza\\appdata\\local\\anaconda3\\lib\\site-packages (from anyio<5,>=3.6.2->starlette>=0.24.0->fiftyone) (3.7)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\lucas.souza\\appdata\\local\\anaconda3\\lib\\site-packages (from anyio<5,>=3.6.2->starlette>=0.24.0->fiftyone) (1.2.0)\n",
      "Collecting hyperframe<7,>=6.1 (from h2>=3.1.0->hypercorn>=0.13.2->fiftyone)\n",
      "  Downloading hyperframe-6.1.0-py3-none-any.whl.metadata (4.3 kB)\n",
      "Collecting hpack<5,>=4.1 (from h2>=3.1.0->hypercorn>=0.13.2->fiftyone)\n",
      "  Downloading hpack-4.1.0-py3-none-any.whl.metadata (4.6 kB)\n",
      "Requirement already satisfied: certifi in c:\\users\\lucas.souza\\appdata\\local\\anaconda3\\lib\\site-packages (from httpx>=0.10.0->universal-analytics-python3<2,>=1.0.1->fiftyone) (2024.7.4)\n",
      "Collecting httpcore==1.* (from httpx>=0.10.0->universal-analytics-python3<2,>=1.0.1->fiftyone)\n",
      "  Downloading httpcore-1.0.9-py3-none-any.whl.metadata (21 kB)\n",
      "Requirement already satisfied: attrs>=19.2.0 in c:\\users\\lucas.souza\\appdata\\local\\anaconda3\\lib\\site-packages (from jsonlines->voxel51-eta<0.15,>=0.14.0->fiftyone) (23.2.0)\n",
      "Collecting texttable (from py7zr->voxel51-eta<0.15,>=0.14.0->fiftyone)\n",
      "  Downloading texttable-1.7.0-py2.py3-none-any.whl.metadata (9.8 kB)\n",
      "Collecting pycryptodomex>=3.20.0 (from py7zr->voxel51-eta<0.15,>=0.14.0->fiftyone)\n",
      "  Downloading pycryptodomex-3.23.0-cp37-abi3-win_amd64.whl.metadata (3.5 kB)\n",
      "Collecting brotli>=1.1.0 (from py7zr->voxel51-eta<0.15,>=0.14.0->fiftyone)\n",
      "  Downloading Brotli-1.1.0-cp311-cp311-win_amd64.whl.metadata (5.6 kB)\n",
      "Collecting pyzstd>=0.16.1 (from py7zr->voxel51-eta<0.15,>=0.14.0->fiftyone)\n",
      "  Downloading pyzstd-0.17.0-cp311-cp311-win_amd64.whl.metadata (2.6 kB)\n",
      "Collecting pyppmd<1.3.0,>=1.1.0 (from py7zr->voxel51-eta<0.15,>=0.14.0->fiftyone)\n",
      "  Downloading pyppmd-1.2.0-cp311-cp311-win_amd64.whl.metadata (5.6 kB)\n",
      "Collecting pybcj<1.1.0,>=1.0.0 (from py7zr->voxel51-eta<0.15,>=0.14.0->fiftyone)\n",
      "  Downloading pybcj-1.0.6-cp311-cp311-win_amd64.whl.metadata (3.8 kB)\n",
      "Collecting multivolumefile>=0.2.3 (from py7zr->voxel51-eta<0.15,>=0.14.0->fiftyone)\n",
      "  Downloading multivolumefile-0.2.3-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting inflate64<1.1.0,>=1.0.0 (from py7zr->voxel51-eta<0.15,>=0.14.0->fiftyone)\n",
      "  Downloading inflate64-1.0.3-cp311-cp311-win_amd64.whl.metadata (4.5 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\lucas.souza\\appdata\\local\\anaconda3\\lib\\site-packages (from requests->voxel51-eta<0.15,>=0.14.0->fiftyone) (3.3.2)\n",
      "Collecting typing-extensions>=4.10.0 (from starlette>=0.24.0->fiftyone)\n",
      "  Downloading typing_extensions-4.14.0-py3-none-any.whl.metadata (3.0 kB)\n",
      "Downloading fiftyone-1.5.2-py3-none-any.whl (10.8 MB)\n",
      "   ---------------------------------------- 0.0/10.8 MB ? eta -:--:--\n",
      "   --------------------------- ------------ 7.3/10.8 MB 45.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  10.7/10.8 MB 41.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 10.8/10.8 MB 19.9 MB/s eta 0:00:00\n",
      "Downloading dacite-1.7.0-py3-none-any.whl (12 kB)\n",
      "Downloading fiftyone_brain-0.21.2-py3-none-any.whl (112 kB)\n",
      "Downloading fiftyone_db-1.1.7-py3-none-win_amd64.whl (20.1 MB)\n",
      "   ---------------------------------------- 0.0/20.1 MB ? eta -:--:--\n",
      "   ------------------ --------------------- 9.4/20.1 MB 58.8 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 12.3/20.1 MB 30.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  19.9/20.1 MB 38.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  19.9/20.1 MB 38.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 20.1/20.1 MB 19.0 MB/s eta 0:00:00\n",
      "Downloading hypercorn-0.17.3-py3-none-any.whl (61 kB)\n",
      "Downloading mongoengine-0.29.1-py3-none-any.whl (112 kB)\n",
      "Downloading motor-3.6.1-py3-none-any.whl (74 kB)\n",
      "Downloading pymongo-4.9.2-cp311-cp311-win_amd64.whl (874 kB)\n",
      "   ---------------------------------------- 0.0/874.7 kB ? eta -:--:--\n",
      "   ---------------------------------------- 874.7/874.7 kB 4.3 MB/s eta 0:00:00\n",
      "Downloading sse_starlette-0.10.3-py3-none-any.whl (8.0 kB)\n",
      "Downloading sseclient_py-1.8.0-py2.py3-none-any.whl (8.8 kB)\n",
      "Downloading starlette-0.47.1-py3-none-any.whl (72 kB)\n",
      "Downloading strawberry_graphql-0.275.2-py3-none-any.whl (305 kB)\n",
      "Downloading universal_analytics_python3-1.1.1-py3-none-any.whl (10 kB)\n",
      "Downloading voxel51_eta-0.14.2-py2.py3-none-any.whl (943 kB)\n",
      "   ---------------------------------------- 0.0/943.0 kB ? eta -:--:--\n",
      "   --------------------------------------- 943.0/943.0 kB 14.5 MB/s eta 0:00:00\n",
      "Downloading opencv_python_headless-4.11.0.86-cp37-abi3-win_amd64.whl (39.4 MB)\n",
      "   ---------------------------------------- 0.0/39.4 MB ? eta -:--:--\n",
      "   ------------ --------------------------- 12.6/39.4 MB 60.7 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 24.4/39.4 MB 57.1 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 35.4/39.4 MB 56.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  39.3/39.4 MB 55.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  39.3/39.4 MB 55.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  39.3/39.4 MB 55.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  39.3/39.4 MB 55.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  39.3/39.4 MB 55.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  39.3/39.4 MB 55.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  39.3/39.4 MB 55.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  39.3/39.4 MB 55.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  39.3/39.4 MB 55.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  39.3/39.4 MB 55.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  39.3/39.4 MB 55.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 39.4/39.4 MB 12.4 MB/s eta 0:00:00\n",
      "Downloading argcomplete-3.6.2-py3-none-any.whl (43 kB)\n",
      "Downloading boto3-1.38.41-py3-none-any.whl (139 kB)\n",
      "Downloading Deprecated-1.2.18-py2.py3-none-any.whl (10.0 kB)\n",
      "Downloading ftfy-6.3.1-py3-none-any.whl (44 kB)\n",
      "Downloading humanize-4.12.3-py3-none-any.whl (128 kB)\n",
      "Downloading pprintpp-0.4.0-py2.py3-none-any.whl (16 kB)\n",
      "Downloading pydash-8.0.5-py3-none-any.whl (102 kB)\n",
      "Downloading xmltodict-0.14.2-py2.py3-none-any.whl (10.0 kB)\n",
      "Downloading anyio-4.9.0-py3-none-any.whl (100 kB)\n",
      "Downloading botocore-1.38.41-py3-none-any.whl (13.7 MB)\n",
      "   ---------------------------------------- 0.0/13.7 MB ? eta -:--:--\n",
      "   ------------------------------------ --- 12.6/13.7 MB 56.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  13.6/13.7 MB 53.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  13.6/13.7 MB 53.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 13.7/13.7 MB 18.3 MB/s eta 0:00:00\n",
      "Downloading dnspython-2.7.0-py3-none-any.whl (313 kB)\n",
      "Downloading graphql_core-3.2.6-py3-none-any.whl (203 kB)\n",
      "Downloading h2-4.2.0-py3-none-any.whl (60 kB)\n",
      "Downloading httpx-0.28.1-py3-none-any.whl (73 kB)\n",
      "Downloading httpcore-1.0.9-py3-none-any.whl (78 kB)\n",
      "Downloading h11-0.16.0-py3-none-any.whl (37 kB)\n",
      "Downloading s3transfer-0.13.0-py3-none-any.whl (85 kB)\n",
      "Downloading wsproto-1.2.0-py3-none-any.whl (24 kB)\n",
      "Downloading jsonlines-4.0.0-py3-none-any.whl (8.7 kB)\n",
      "Downloading priority-2.0.0-py3-none-any.whl (8.9 kB)\n",
      "Downloading py7zr-1.0.0-py3-none-any.whl (69 kB)\n",
      "Downloading rarfile-4.2-py3-none-any.whl (29 kB)\n",
      "Downloading tzlocal-5.3.1-py3-none-any.whl (18 kB)\n",
      "Downloading Brotli-1.1.0-cp311-cp311-win_amd64.whl (357 kB)\n",
      "Downloading hpack-4.1.0-py3-none-any.whl (34 kB)\n",
      "Downloading hyperframe-6.1.0-py3-none-any.whl (13 kB)\n",
      "Downloading inflate64-1.0.3-cp311-cp311-win_amd64.whl (35 kB)\n",
      "Downloading multivolumefile-0.2.3-py3-none-any.whl (17 kB)\n",
      "Downloading pybcj-1.0.6-cp311-cp311-win_amd64.whl (24 kB)\n",
      "Downloading pycryptodomex-3.23.0-cp37-abi3-win_amd64.whl (1.8 MB)\n",
      "   ---------------------------------------- 0.0/1.8 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.8/1.8 MB 16.4 MB/s eta 0:00:00\n",
      "Downloading pyppmd-1.2.0-cp311-cp311-win_amd64.whl (46 kB)\n",
      "Downloading pyzstd-0.17.0-cp311-cp311-win_amd64.whl (246 kB)\n",
      "Downloading typing_extensions-4.14.0-py3-none-any.whl (43 kB)\n",
      "Downloading texttable-1.7.0-py2.py3-none-any.whl (10 kB)\n",
      "Installing collected packages: texttable, sseclient-py, pprintpp, brotli, xmltodict, tzlocal, typing-extensions, rarfile, pyppmd, pycryptodomex, pybcj, priority, opencv-python-headless, multivolumefile, jsonlines, inflate64, hyperframe, humanize, hpack, h11, graphql-core, ftfy, fiftyone-db, dnspython, Deprecated, dacite, argcomplete, wsproto, strawberry-graphql, pyzstd, pymongo, pydash, httpcore, h2, botocore, anyio, starlette, s3transfer, py7zr, motor, mongoengine, hypercorn, httpx, fiftyone-brain, voxel51-eta, universal-analytics-python3, sse-starlette, boto3, fiftyone\n",
      "  Attempting uninstall: brotli\n",
      "    Found existing installation: Brotli 1.0.9\n",
      "    Uninstalling Brotli-1.0.9:\n",
      "      Successfully uninstalled Brotli-1.0.9\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing_extensions 4.12.2\n",
      "    Uninstalling typing_extensions-4.12.2:\n",
      "      Successfully uninstalled typing_extensions-4.12.2\n",
      "  Attempting uninstall: botocore\n",
      "    Found existing installation: botocore 1.31.64\n",
      "    Uninstalling botocore-1.31.64:\n",
      "      Successfully uninstalled botocore-1.31.64\n",
      "  Attempting uninstall: anyio\n",
      "    Found existing installation: anyio 3.5.0\n",
      "    Uninstalling anyio-3.5.0:\n",
      "      Successfully uninstalled anyio-3.5.0\n",
      "Successfully installed Deprecated-1.2.18 anyio-4.9.0 argcomplete-3.6.2 boto3-1.38.41 botocore-1.38.41 brotli-1.1.0 dacite-1.7.0 dnspython-2.7.0 fiftyone-1.5.2 fiftyone-brain-0.21.2 fiftyone-db-1.1.7 ftfy-6.3.1 graphql-core-3.2.6 h11-0.16.0 h2-4.2.0 hpack-4.1.0 httpcore-1.0.9 httpx-0.28.1 humanize-4.12.3 hypercorn-0.17.3 hyperframe-6.1.0 inflate64-1.0.3 jsonlines-4.0.0 mongoengine-0.29.1 motor-3.6.1 multivolumefile-0.2.3 opencv-python-headless-4.11.0.86 pprintpp-0.4.0 priority-2.0.0 py7zr-1.0.0 pybcj-1.0.6 pycryptodomex-3.23.0 pydash-8.0.5 pymongo-4.9.2 pyppmd-1.2.0 pyzstd-0.17.0 rarfile-4.2 s3transfer-0.13.0 sse-starlette-0.10.3 sseclient-py-1.8.0 starlette-0.47.1 strawberry-graphql-0.275.2 texttable-1.7.0 typing-extensions-4.14.0 tzlocal-5.3.1 universal-analytics-python3-1.1.1 voxel51-eta-0.14.2 wsproto-1.2.0 xmltodict-0.14.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~ip (C:\\Users\\lucas.souza\\AppData\\Local\\anaconda3\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~umpy (C:\\Users\\lucas.souza\\AppData\\Local\\anaconda3\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ip (C:\\Users\\lucas.souza\\AppData\\Local\\anaconda3\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~umpy (C:\\Users\\lucas.souza\\AppData\\Local\\anaconda3\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ip (C:\\Users\\lucas.souza\\AppData\\Local\\anaconda3\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~umpy (C:\\Users\\lucas.souza\\AppData\\Local\\anaconda3\\Lib\\site-packages)\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "aiobotocore 2.7.0 requires botocore<1.31.65,>=1.31.16, but you have botocore 1.38.41 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "!pip install fiftyone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "b5miB4VWh2qG"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.5.2'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import fiftyone\n",
    "from fiftyone.zoo import load_zoo_dataset\n",
    "fiftyone.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "fh5Kb87bJgM_"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.9.2'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.pyplot as plt\n",
    "matplotlib.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "m2VBCkMaD_Oq"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.26.4'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "i4vTK4T_OJbh"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.3.0'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "uIXSa-wkhoAe"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.12.2'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "sns.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "KkyhrEtWbb93"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.5.1'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sklearn\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sklearn.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "oCW0Dh0FiFu4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'4.52.4'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import transformers\n",
    "from transformers import CLIPModel, CLIPProcessor\n",
    "transformers.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "EeUsctVgX9ZD"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.7.1+cpu'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t90ER3kVifs2"
   },
   "source": [
    "# Sistemas lineares"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kyD2vl0Zi5hO"
   },
   "source": [
    "Vamos começar importando um dataset real, que relaciona o preço de casas na Califórnia com algumas de suas características. [Mais detalhes aqui!](https://scikit-learn.org/stable/datasets/real_world.html#california-housing-dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "m1Ix18ascXCx"
   },
   "outputs": [],
   "source": [
    "A, y = fetch_california_housing(return_X_y=True, as_frame=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a_s2ZFMTjZJK"
   },
   "source": [
    "Exiba as variáveis `A` e `y`, para ver que elas foram importadas em formato de dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lSkCAGQAcnO-"
   },
   "outputs": [],
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4qXkF3tpcp69"
   },
   "outputs": [],
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cOhpTR1sjlpl"
   },
   "source": [
    "Exiba também em formato de NumPy array, utilizando o método apropriado em cada dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VlFuG9enfVI1"
   },
   "outputs": [],
   "source": [
    "A.___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pwB4C9x8fZkP"
   },
   "outputs": [],
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BAkrXzt3j5i7"
   },
   "source": [
    "Agora vamos supor que cada atributo das casas tem uma relação linear com o preço, de forma que as matrizes `A` e `y` podem ser usadas para descobrir qual é o preço, por unidade, de cada atributo `x`.\n",
    "\n",
    "Ou seja, esses dados podem ser representados como um sistema linear do tipo $y = Ax$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uNYCkDcpkiwZ"
   },
   "source": [
    "Comece determinando a matriz inversa de `A`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "az2KilNXcqqi"
   },
   "outputs": [],
   "source": [
    "A_inv = ___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1exbp8FTkmZQ"
   },
   "source": [
    "Agora determine os valores de `x`, fazendo a multiplicação matricial de `A_inv` com `y`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qByUgBTIdOsE"
   },
   "outputs": [],
   "source": [
    "x = ___\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "shO0tThukyU-"
   },
   "source": [
    "Para facilitar a interpretação, arredonde os valores de `x` para 2 casas decimais."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CGAdvJMxe33S"
   },
   "outputs": [],
   "source": [
    "x.___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EeS2RX7ilO17"
   },
   "source": [
    "Agora, calcule uma variável `y_pred`, que é obtida multiplicado `A` por `x`. Essa variável representa o valor das casas previsto com base nos atributos que possui. Se nossa suposição de que a relação entre os atributos e o preço das casas é perfeitamente linear fosse verdadeiro, então os valores de `y_pred` e `y` (o preço real) seriam idênticos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0PeR22jodTr5"
   },
   "outputs": [],
   "source": [
    "y_pred = ___\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OUJ0VrMvljBP"
   },
   "source": [
    "Compare os preços previstos com os preços reais. Como este dataset é muito extenso, exiba somente os 100 primeiros valores.\n",
    "\n",
    "Para isso, use a função `plt.plot` duas vezes, uma para cada variável. Use o parâmetro `label` para adicionar um rótulo a cada plot, e no final a função `plt.legend` para exibir os rótulos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lI6paDL-eUes"
   },
   "outputs": [],
   "source": [
    "plt.plot(y[:100], label=___)\n",
    "plt.plot(___)\n",
    "plt.___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hzSJh74Cl_7k"
   },
   "source": [
    "Observe que, apesar de alguns valores bastante errados, ainda assim os preços preditos seguem a tendência mostrada pelos preços reais. Ou seja, nossa suposição inicial não estava tão errada assim!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8frvz2UfmPV8"
   },
   "source": [
    "# Redes neurais"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c0-gg5uNmUOo"
   },
   "source": [
    "Vamos agora tentar outra abordagem, implementando uma rede neural simples. A vantagem das redes neurais é que elas são capazes de estimar valores mesmo quando a relação não é linear."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pBGTQ-HZmjKN"
   },
   "source": [
    "Exiba a matriz `A` de novo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UlniMnEGWW0y"
   },
   "outputs": [],
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wwl3oFTcWYlg"
   },
   "source": [
    "Veja que os valores estão em diferentes ordens de grandeza. O desempenho das redes neurais é bastante prejudicado nestas situações. Por isso, é prática comum normalizar os valores antes de passá-los para a rede.\n",
    "\n",
    "Comece instanciando um scaler do tipo `StandardScaler`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fEzmLYwCd_GC"
   },
   "outputs": [],
   "source": [
    "scaler = ___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8jJ4LWkIm_rd"
   },
   "source": [
    "Agora utilize o scaler para escalonar os dados de `A` e `y`. No caso de `y`, como o scaler espera receber um array com 2 dimensões, nós precisamos primeiro recuperar o array de `y`, depois aplicar o método `reshape` para transformá-lo em um vetor coluna, e no final podemos aplicar `reshape` de novo para ele voltar a ser um array 1-D (vetor)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I6S0yDY6nAJF"
   },
   "outputs": [],
   "source": [
    "A_scaled = scaler.fit_transform(___)\n",
    "y_scaled = ___(___.___.___).___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iX5mvYj0nqQs"
   },
   "source": [
    "Agora crie 3 camadas de uma rede neural: a primeira com 20 neurônios, a segunda também com 20, e a terceira sendo nossa camada de saída. Neste caso, como a variável de saída é numérica, este é um problema de regressão, portanto a camada de saída deve ter um único neurônio.\n",
    "\n",
    "Não se esqueça de, antes de tudo, fixar a semente de números aleatórios! Utilize a semente = `6`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KSC7vC0tf7fi"
   },
   "outputs": [],
   "source": [
    "np.___\n",
    "\n",
    "W_1 = np.random.randn(___)\n",
    "b_1 = ___\n",
    "\n",
    "W_2 = ___\n",
    "b_2 = ___\n",
    "\n",
    "W_3 = ___\n",
    "b_3 = ___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hd8mzJHNozUu"
   },
   "source": [
    "Agora calcule os valores de saída após cada camada. Para ter certeza de que está tudo certo, exiba o `shape` de cada saída logo após calculá-la."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T6_ACmIFgbmk"
   },
   "outputs": [],
   "source": [
    "out_1 = ___\n",
    "out_1.___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "96BsvJ8lgfbU"
   },
   "outputs": [],
   "source": [
    "___\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xDgP5wwUghuI"
   },
   "outputs": [],
   "source": [
    "___\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zGV8dqHhpEY9"
   },
   "source": [
    "Assim como no exercício dos Sistemas lineares, compare os valores da camada de saída com os valores reais. Neste caso, temos que comparar com os valores escalonados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2YRLeeHvglYo"
   },
   "outputs": [],
   "source": [
    "plt.plot(___, label=___)\n",
    "___\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oiHfiWM5pUOe"
   },
   "source": [
    "Observe que, neste caso, os valores preditos são bem diferentes dos valores reais. De fato, são tão extremos que os valore reais parecem achatados. Isso é normal, já que as redes neurais começam sem qualquer conhecimento, sendo refinadas ao longo do processo de treinamento. No final, a expectativa é que o erro seja o menor possível, e por consequência, as duas linhas sejam o mais idênticas possível também."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BcuB41XFpuw4"
   },
   "source": [
    "Para ter uma ideia do tamanho do erro nessa primeira iteração da rede neural, calcule o erro como a diferença entre os valores, e exiba sua média."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ECeG9yO_g460"
   },
   "outputs": [],
   "source": [
    "error = ___\n",
    "error.___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_8KxSdkrqBHF"
   },
   "source": [
    "Isso significa que, para cada casa, a rede neural errou em média em 2.63 pontos na escala escalonada de `y`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2ICfhOMRqKrI"
   },
   "source": [
    "# Eigendecomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rNEob6OzzkXP"
   },
   "source": [
    "Neste exercício nós vamos vamos aplicar a técnica de eigendecomposition a textos.\n",
    "\n",
    "Vamos começar gerando duas coleções de 25 frases cada, uma relacionada a tecnologia, outra relacionada a esportes. Depois, juntamos as duas coleções em uma única lista."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EJ-lYKYax_BT"
   },
   "outputs": [],
   "source": [
    "corpus_technology = [\n",
    "    \"A inovação tecnológica tem sido impulsionada pelo processamento rápido de grandes volumes de dados, permitindo avanços notáveis.\",\n",
    "    \"O uso de redes integradas facilita a automatização de processos e melhora a eficiência na análise de dados.\",\n",
    "    \"As empresas estão cada vez mais voltadas para a inovação, investindo em processamento avançado e na coleta de dados em larga escala.\",\n",
    "    \"Com o crescimento das redes digitais, o processamento de dados em tempo real tornou-se essencial para a competitividade das empresas.\",\n",
    "    \"A automatização de tarefas rotineiras permite que os profissionais foquem em inovação, gerando novas maneiras de analisar dados.\",\n",
    "    \"A inovação em sistemas de processamento de dados tornou possível o uso de redes neurais para resolver problemas complexos.\",\n",
    "    \"A coleta e processamento de dados em redes descentralizadas ajudam a garantir maior segurança e confiabilidade nas transações digitais.\",\n",
    "    \"A automatização de processos depende da integração de redes eficientes e do acesso rápido aos dados necessários.\",\n",
    "    \"A inovação digital exige um processamento de dados rápido, principalmente em redes que atendem a milhares de usuários simultaneamente.\",\n",
    "    \"A automatização permite que grandes volumes de dados sejam processados de forma ágil e eficiente, aprimorando o desempenho da rede.\",\n",
    "    \"Os avanços no processamento de dados têm possibilitado o desenvolvimento de redes mais seguras e confiáveis para troca de informações.\",\n",
    "    \"A inovação está presente em todos os setores que utilizam processamento de dados em tempo real, especialmente nas redes empresariais.\",\n",
    "    \"A automatização vem se tornando mais acessível com o avanço das redes e da capacidade de processamento de dados.\",\n",
    "    \"A análise de grandes volumes de dados torna-se mais eficaz com redes interligadas e processamentos otimizados.\",\n",
    "    \"Com o aumento da inovação tecnológica, novas redes de processamento de dados surgem para suportar a demanda crescente.\",\n",
    "    \"A automatização de processos industriais depende de redes avançadas e do rápido processamento de dados para manter a eficiência.\",\n",
    "    \"A inovação nas redes digitais e o processamento ágil de dados permitem o desenvolvimento de novas ferramentas de gestão.\",\n",
    "    \"A automatização e o uso de redes conectadas são fundamentais para o processamento eficiente de dados no ambiente corporativo.\",\n",
    "    \"O avanço na inovação tecnológica tem permitido que redes mais seguras realizem processamento de dados de forma eficaz.\",\n",
    "    \"A integração entre redes de dados e a automatização dos sistemas tem otimizado a operação de diversas indústrias.\",\n",
    "    \"O uso de redes digitais promove a inovação e melhora o processamento de dados, facilitando a tomada de decisões.\",\n",
    "    \"A automatização torna-se cada vez mais eficiente com redes e processamento de dados em nuvem, otimizando o armazenamento.\",\n",
    "    \"A inovação em redes de dados e o processamento em alta velocidade são essenciais para o sucesso de projetos de grande escala.\",\n",
    "    \"A automatização dos processos e o processamento de grandes volumes de dados ajudam as empresas a expandir sua rede de atuação.\",\n",
    "    \"As redes modernas permitem a coleta e o processamento de dados em tempo real, promovendo a inovação na experiência do usuário.\"\n",
    "]\n",
    "\n",
    "corpus_sports = [\n",
    "    \"A disciplina é um dos fatores mais importantes para que o atleta consiga um desempenho de excelência contra seus adversários.\",\n",
    "    \"Para alcançar a vitória, o atleta precisa de uma tática sólida e da disciplina em manter seu treinamento.\",\n",
    "    \"A análise das táticas do adversário ajuda o atleta a preparar melhor sua própria estratégia para aumentar o desempenho.\",\n",
    "    \"A vitória só é alcançada quando o atleta alia disciplina com uma tática eficiente para vencer o adversário.\",\n",
    "    \"O desempenho do atleta em uma competição é reflexo direto da disciplina e do foco em aprimorar suas táticas.\",\n",
    "    \"Contra adversários experientes, a vitória depende de uma combinação de disciplina e ajustes rápidos de tática.\",\n",
    "    \"Para enfrentar um adversário à altura, é necessário muito treinamento e disciplina, além de desenvolver boas táticas.\",\n",
    "    \"A vitória não vem sem esforço; é preciso disciplina para melhorar o desempenho e planejar táticas eficazes.\",\n",
    "    \"A disciplina no treinamento permite que o atleta mantenha seu melhordesempenho ao longo de toda a competição.\",\n",
    "    \"Estudar o estilo do adversário faz parte da preparação tática para maximizar o desempenho e buscar a vitória.\",\n",
    "    \"A tática em jogo e a disciplina são essenciais para superar o desempenho do adversário em partidas decisivas.\",\n",
    "    \"A vitória requer não apenas habilidade física, mas também uma estratégia que use a tática certa contra cada adversário.\",\n",
    "    \"Com disciplina, o atleta desenvolve sua tática e melhora o desempenho, tornando-se um adversário mais difícil.\",\n",
    "    \"A tática e a disciplina no treino constante são os principais fatores que garantem um desempenho sólido em competições.\",\n",
    "    \"A vitória é resultado de um bom desempenho, que se constrói com muita disciplina e uma tática bem planejada.\",\n",
    "    \"Estudar o adversário é fundamental para desenvolver uma tática que melhore o desempenho e aumente as chances de vitória.\",\n",
    "    \"A disciplina permite ao atleta aperfeiçoar suas táticas e preparar-se para qualquer adversário que encontrar.\",\n",
    "    \"Para garantir um bom desempenho, o atleta precisa de uma tática que considere as possíveis jogadas do adversário.\",\n",
    "    \"A vitória em competições exige disciplina constante e o desenvolvimento de uma tática eficaz contra adversários fortes.\",\n",
    "    \"O desempenho do atleta depende de sua capacidade de adaptar a tática conforme a estratégia do adversário.\",\n",
    "    \"A disciplina nos treinos permite que o atleta encontre a tática ideal para superar o desempenho do adversário.\",\n",
    "    \"Com disciplina e determinação, o atleta aprimora seu desempenho e desenvolve táticas para cada tipo de adversário.\",\n",
    "    \"A vitória é o resultado de uma combinação de disciplina, tática e um desempenho consistente contra qualquer adversário.\",\n",
    "    \"Conhecer a tática do adversário é um diferencial que ajuda o atleta a planejar sua própria estratégia para a vitória.\",\n",
    "    \"A disciplina ajuda o atleta a aprimorar suas habilidades e buscar a vitória mesmo contra adversários difíceis.\"\n",
    "]\n",
    "\n",
    "corpus = corpus_technology + corpus_sports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mFIuDJicz_5_"
   },
   "source": [
    "Agora nós temos que representar esses textos em uma forma numérica. Para isso, vamos utilizar a técnica de vectorização TF-IDF. Basicamente, esta técnica cria uma coluna para cada palavra que aparece no texto, e cada frase recebe uma pontuação para indicar que ela contém a palavra. Esse score é afetado para refletir se a palavra aparece em muitas ou poucas frases, já que, se aparecer em poucas, ela tem, teoricamente, maior potencial para diferenciar as frases. Para mais detalhes sobre a técnica, [acesse aqui](https://scikit-learn.org/stable/modules/feature_extraction.html#tfidf-term-weighting)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x8UMYDGOyEMy"
   },
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "X = X.toarray()\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QMvN49wm1Do_"
   },
   "source": [
    "Veja que o array `X` contém 50 linhas, correspondente às 50 frases, e 285 colunas, correspondente às 285 palavras que aparecem nos textos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5hUvSjcl2XLh"
   },
   "source": [
    "Por curiosidade, nós podemos inspecionar qual é o vocabulário que o `vectorizer` encontrou, junto com seu índice correspondente em `X`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WXSesL7r2N88"
   },
   "outputs": [],
   "source": [
    "vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yMYfdvB82Ewa"
   },
   "source": [
    "Agora vamos inspecionar esses valores, nas 4 primeiras frases de cada categoria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z9XD0v-E0leN"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, 4, figsize=(10, 3))\n",
    "for i in range(4):\n",
    "    ax[0, i].imshow(X[i].reshape(15, 19), cmap='gray') # este `reshape` só tem como objetivo melhorar a visualização, para transformar o vetor em uma matriz retangular.\n",
    "    ax[0, i].set_title('Tecnologia')\n",
    "    ax[0, i].axis('off')\n",
    "    ax[1, i].imshow(X[25+i].reshape(15, 19), cmap='gray') # as frases de esporte começam no índice 25.\n",
    "    ax[1, i].set_title('Esportes')\n",
    "    ax[1, i].axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YDN5lYld1jdm"
   },
   "source": [
    "Perceba que, primeiro, a maioria dos \"pixels\" nessa visualização são pretos, o que indica que as frases **não têm** a palavra correspondente (o score é 0). Segundo, os demais pixels são algum tom de cinza, o que indica um valor entre 0 e 1 (branco). Isso indica o score para cada palavra que aparece na frase. Terceiro, observe que não existe uma diferença visível entre os textos de tecnologia e esportes. É por isso que vamos aplicar a técnica de eigendecomposition, tentando encontrar os eigenvectors mais influentes na composição desse dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BnLi_SVV3FnU"
   },
   "source": [
    "Agora é sua vez! Calcule a matriz de covarância para o array `X`, que representa o dataset. Lembre-se que essa matriz precisa correlacionar os scores de TF-IDF; você precisa fazer alguma transformação em `X` antes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EEQ3dQp-3e3z"
   },
   "outputs": [],
   "source": [
    "X_cov = ___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "odrN69hb3o5D"
   },
   "source": [
    "Que tal agora visualizar essa matriz no formato de um heatmap?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-Akxbo1F3uED"
   },
   "outputs": [],
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vClpXEnF36nv"
   },
   "source": [
    "Essa matriz sozinha não é muito informativa..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ej8dtdUL3-1h"
   },
   "source": [
    "Agora, aplique a técnica de eigendecomposition na matriz de covariância. Use a função adequada para garantir que os valores sejam reais."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0F4lPLc44Is0"
   },
   "outputs": [],
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EtF8MHo24PV-"
   },
   "source": [
    "A próxima etapa é ordenar os eigenvalues em `Q` e os eigenvalues em `l` em função do valor dos eigenvalues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Jo1zIJv94aGA"
   },
   "outputs": [],
   "source": [
    "___\n",
    "___\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u44wlwRD4hvn"
   },
   "source": [
    "Crie um plot no formato de barras para visualizar os primeiros 10 eigenvalues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O6Kt2_Yj8VFS"
   },
   "outputs": [],
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B75qQ3zg4rHX"
   },
   "source": [
    "Isso nos mostra que o primeiro eigenvalue (e seu respectivo eigenvector) tem praticamente o dobro da influência do segundo, o que indica que ele pode ser suficiente para caracterizar esse dataset. Será?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dBkp49AK5ItG"
   },
   "source": [
    "Você pode agora visualizar as primeiras 5 \"eigenphrases\" como plots. Não se esqueça de aplicar o mesmo `reshape` que aplicamos para visualizar as frases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yGANAFagyrwh"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 5, figsize=(12, 6))\n",
    "for ___:\n",
    "    ___\n",
    "    ___.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AK9LehJN9mam"
   },
   "source": [
    "Veja que o \"perfil\" dessas \"eigenphrases\" é bem distinto, o que era de se esperar já que eles são os que melhor caracterizam esse dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dppa_SSh991W"
   },
   "source": [
    "Agora, projete os textos (representados pelo array `X`) no espaço das eigenphrases. Na sequência, plote essas projeções para as primeiras 4 frases de cada categoria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oHxfdhiMyW9x"
   },
   "outputs": [],
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GXaqmjARyiAv"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, 4, figsize=(10, 3))\n",
    "for ___:\n",
    "    ___\n",
    "    ___.set_title(___)\n",
    "    ___.axis('off')\n",
    "    ___\n",
    "    ___\n",
    "    ___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CeC2okb_-XDt"
   },
   "source": [
    "Plote de novo, mas agora dando um zoom apenas nos 15 primeiros \"pixels\". Perceba que você vai ter que adequar o parâmetro de `reshape` para refletir que agora são só 15 pixels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X8utqvcu3t7h"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, 4, figsize=(12, 6))\n",
    "for ___:\n",
    "    ___\n",
    "    ___\n",
    "    ___\n",
    "    ___\n",
    "    ___\n",
    "    ___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7Ruo6zHj_cK8"
   },
   "source": [
    "Observe que, de fato, o primeiro pixel se revela o mais informativo. Nas frases de tecnologia, ele é mais claro, revelando um score baixo; nas frases de esportes, ele é mais escuro, correspondente a um score alto. O segundo pixel também carrega uma parcela de informação; com exceção da terceira frase nas duas categorias, também parece haver um padrão: tecnologia mais escuro, esporte mais claro."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H00MoHU9ZtnI"
   },
   "source": [
    "Só por curiosidade, o plot abaixo mostra os 15 primeiros pixels (colunas) das 25 frases (linhas) de tecnologia e de esportes. Observe que, de fato, o primeiro pixel é extremamente informativo, sendo discriminativo da categoria da frase para todos os casos!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TQ1s1Qz7YgCU"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(12, 6))\n",
    "ax[0].imshow(X_projected[:25, :15], cmap='gray')\n",
    "ax[0].set_title('Tecnologia')\n",
    "ax[0].axis('off')\n",
    "ax[1].imshow(X_projected[25:, :15], cmap='gray')\n",
    "ax[1].set_title('Esportes')\n",
    "ax[1].axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K2QiaFd9BD_v"
   },
   "source": [
    "# Singular value decomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-U589M92L7lG"
   },
   "source": [
    "Desta vez, você vai aplicar a técnica singular value decomposition para compactar uma imagem colorida."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UujwhQ7YDRrK"
   },
   "source": [
    "Vamos começar baixando uma imagem nova."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-7ZhuqoiLmkL"
   },
   "outputs": [],
   "source": [
    "!wget https://upload.wikimedia.org/wikipedia/commons/0/0f/Greater_white-fronted_goose_in_flight-1045.jpg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5Md-VXiHAZ3H"
   },
   "source": [
    "© [Imagem](https://en.wikipedia.org/wiki/File:Greater_white-fronted_goose_in_flight-1045.jpg) | Frank Schulenburg | [CC BY-SA 4.0](https://creativecommons.org/licenses/by-sa/4.0/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6PHV6rIeDYIa"
   },
   "source": [
    "Use a biblioteca `mpimg` para visualizar a imagem. Printe também o formato (`shape`) da versão em array da imagem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gpOhYempeFB5"
   },
   "outputs": [],
   "source": [
    "image = mpimg.___\n",
    "print(image.___)\n",
    "plt.___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wc9L71SODgQz"
   },
   "source": [
    "Observe que esta imagem é ainda maior que a anterior: 2851 x 3689 pixels. Isso resulta em $2851 \\times 3689 \\times 3 = 31.552.017$ bytes necessários para armazená-la. Vamos ver como a compactação se sai!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AIduP3Q4DxrC"
   },
   "source": [
    "A primeira coisa que você vai precisar fazer é transformar a imagem, que agora é um tensor com 3 dimensões, em uma matriz 2D. Para isso, você pode simplesmente aplicar `reshape` para transformar as duas últimas dimensões em uma só. Você pode informar a `reshape`, como primeiro parâmetro, o número de pixels horizontais da imagem (ou `image.shape[0]`), e como segundo, `-1`. Com `-1`, o próprio NumPy se encarrega de calcular o tamanho daquela dimensão.\n",
    "\n",
    "Faça isso, e depois separe o `shape` do array que representa a imagem nas variáveis `w` e `h`. Printe essas variáveis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7hFEhx7fXPM8"
   },
   "outputs": [],
   "source": [
    "image = image.reshape(___, ___)\n",
    "___\n",
    "print(___, ___)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dldbiDczEmVX"
   },
   "source": [
    "Observe que o array continua tendo 2851 linhas, mas agora, temos 11067 colunas, ou seja, as 3689 colunas originais x 3 canais de cor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PZ4Iv7WeE0Z_"
   },
   "source": [
    "Agora, aplique a técnica SVD para decompor este array. Aqui, você também vai precisar usar o parâmetro `full_matrices=False`. Para garantir que está tudo dando certo, printe os `shape`s de `U`, `S` e `Vh`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "932NAwFB7_Z5"
   },
   "outputs": [],
   "source": [
    "___\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DFIFnDEiFRmY"
   },
   "source": [
    "Visualize as reconstruções utilizando apenas os primeiros valores singulares. Como neste exemplo a imagem é maior, plote 10 imagens reconstruídas com valores singulares de 1 até 50, tomados 5 a 5. Após a reconstrução, você também terá que fazer o `reshape` das imagens, para voltar a separar a dimensão das cores. Para isso, você pode usar `reshape(w, -1, 3)`. Aqui você também vai ter que converter os valores dos pixels para números inteiros com `astype(int)`. Não deixe de colocar no título de cada imagem a quantidade de dados necessários para armazenar cada reconstrução."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xze1HGL4_QIf"
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 5, figsize=(20, 7))\n",
    "for i, n in enumerate(range(___)):\n",
    "    ___\n",
    "    ___\n",
    "    ___\n",
    "    ___\n",
    "    ___\n",
    "    ___.imshow(___.reshape(___).astype(int))\n",
    "    ___\n",
    "    ___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ISrN10zpG-Iq"
   },
   "source": [
    "(Não se preocupe com o WARNING, ele é exibido porque, na reconstrução, por questões de arredondamento, alguns valores saem ligeiramente do intervalo $[0, 1]$, então a biblioteca precisa corrigi-los para ficar dentro deste intervalo.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1YY9NDeeHSHI"
   },
   "source": [
    "Observe que, mesmo utilizando 46 valores singulares, a imagem compactada ainda apresenta alguns artefatos. Entretanto, a economia de dados é enorme. Calcule a economia percentual resultante."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qJbOxGqZG9Z0"
   },
   "outputs": [],
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1l0dvuzLHuML"
   },
   "source": [
    "Ou seja, a economia é de 98%!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jJlbzyprG1xM"
   },
   "source": [
    "# PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DUB5f53MH5lg"
   },
   "source": [
    "Para o exercício de PCA, você vai trabalhar com um dataset mais desafiador, o mesmo dataset dos preços das casas utilizado nos 3 primeiros exercícios."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HWUVBJOXIY1O"
   },
   "source": [
    "Comece fazendo a normalização dos atributos previsores (matriz `A`) utilizando a técnica de escalonamento. Depois, coloque os dados escalonados em um DataFrame, adicione a variável alvo (`y`) com o mesmo nome original, e exiba o DataFrame resultante."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qV0M9LaIFdo4"
   },
   "outputs": [],
   "source": [
    "scaler = ___\n",
    "A_scaled = ___\n",
    "A_scaled = pd.DataFrame(___)\n",
    "housing_scaled = ___\n",
    "___\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wJ-5fABVI1Y3"
   },
   "source": [
    "Desta vez, nós temos 8 variáveis preditoras. O PCA revela seu potencial justamente quando o número de variáveis é maior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZHwRMNaHJESr"
   },
   "source": [
    "Comece exibindo um \"pairplot\" para inspecionar a correlação dessas 8 variáveis entre si. Use a variável alvo como cor do plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z6sZvkqlFf__"
   },
   "outputs": [],
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tF8TX6s5LVHj"
   },
   "source": [
    "Devido à quantidade de variáveis, as correlações são as mais variadas. Observamos pelo menos uma correlação linear (AveRooms x AveBedrms), uma variável que parece não ser afetadas pelas demais (AveOccup), a presença de alguns outliers em algumas comparações (AveRooms, AveBedrms, Population, AveOccup), e também uma curiosidade: o plot Latitude x Longitude representa exatamente o mapa da região geográfica coberta, como era de se esperar. Também observe que, em algumas correlações, a cor que representa o preço das casas apresenta um claro gradiente, ou seja, o preço das casas tende a aumentar conforme o valor de uma das variáveis, ou de ambas, varia."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CfYa4dYJNzpU"
   },
   "source": [
    "Agora aplique o PCA para gerar os PCs deste dataset. Novamente, apresente os dados transformados em um DataFrame, com o nome das colunas sendo o nome dos PCs correspondentes. Lembre-se, como temos 8 variáveis, teremos 8 PCs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y-RPv2bmFux0"
   },
   "outputs": [],
   "source": [
    "pca = ___\n",
    "pca.___(___)\n",
    "A_scaled_pca = ___\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2aeTQ-rVONbH"
   },
   "source": [
    "Gere um novo DataFrame contendo os dados dos PCs, mais a variável alvo. Depois, plote as correlações entre os PCs, usando o preço das casas como escala de cor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x-0ebsNyIW4d"
   },
   "outputs": [],
   "source": [
    "housing_scaled_pca = ___\n",
    "___\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CX6X8AHPOnmD"
   },
   "source": [
    "Veja que, até a inclusão de PC5, parece haver alguma correlação entre os PCs. Depois disso, pouca informação é adicionada. Observe também que os gradientes de cor sumiram. Isto é um resultado comum do PCA, já que ele não leva em consideração a variável alvo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wesGbs_AOwNo"
   },
   "source": [
    "Agora exiba a variância explicada por cada PC, e depois sua soma acumulada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sP8ZjTrJIc1k"
   },
   "outputs": [],
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rk0sfT3vWU14"
   },
   "outputs": [],
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3aU0FQAaO3hf"
   },
   "source": [
    "Observe que, de fato, até PC5 nós temos 90% da variância acumulada. PC6 ainda adiciona mais 8% de variância, mas os dois últimos PCs têm apenas os 2% de variância remanescentes. Isso indica que esse dataset poderia ser representado com no máximo 6 PCs, já que os últimos 2 são pouco informativos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9nolZqfhcAz6"
   },
   "source": [
    "Ainda assim, são precisos 6 PCs para representar 8 variáveis. Alguém pode argumentar que a economia não é tão grande assim. Isto aconteceu porque, como nós vimos nos pairplots das variáveis, havia pouca correlação entre elas. Como uma das consequências do PCA é incorporar as correlações no eixos, havia pouco a incorporar, por isso são necessários mais PCs para manter a variância original do dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1ymXSF3jPTLy"
   },
   "source": [
    "Para visualizar esses efeitos isolados, faça um *scatterplot* de PC1 x PC2, e depois de PC6 x PC7. Novamente, use a variável alvo `y` como gradiente de cor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YfPFYiJnJUsi"
   },
   "outputs": [],
   "source": [
    "___\n",
    "___\n",
    "___\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z4yBVjA4V-T_"
   },
   "outputs": [],
   "source": [
    "___\n",
    "___\n",
    "___\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PxIqlliOPlMi"
   },
   "source": [
    "A correlação de PC1 x PC2 é evidente, mas PC6 x PC7 parece não ter qualquer correlação evidente, o que confirma nossas conclusões anteriores. Observe mais uma vez que o gradiente de cor foi \"destruído\" durante o processo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Du81V_9_P8z2"
   },
   "source": [
    "# Semelhança entre dados estruturados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Md-WTUIEP_h_"
   },
   "source": [
    "Para este exercício, você vai comparar imagens, utilizando um dataset chamado [Coco 2017](https://github.com/voxel51/coco-2017). Como exemplo, vamos utilizar apenas 100 imagens do split de teste desse dataset. Vamos utilizar a biblioteca `fiftyone` para baixar esses dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k6NuB-RHZ9A7"
   },
   "outputs": [],
   "source": [
    "import fiftyone as fo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "--GhpHYHaF9n"
   },
   "outputs": [],
   "source": [
    "dataset = fo.zoo.load_zoo_dataset(\"coco-2017\", split=\"test\", overwrite=True, max_samples=100, shuffle=True, seed=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YDieEcg7QUEM"
   },
   "source": [
    "O objeto `dataset` contém somente o caminho das imagens baixadas. Então, vamos coletar as imagens propriamente ditas em uma lista."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d0H68EZaanDj"
   },
   "outputs": [],
   "source": [
    "images = []\n",
    "for sample in dataset:\n",
    "    images.append(mpimg.imread(sample.filepath))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qeKFpQLcQb_h"
   },
   "source": [
    "Agora vamos visualizar as 5 primeiras imagens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V1wyNePVe0RA"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 5, figsize=(20, 3))\n",
    "for i in range(5):\n",
    "    ax[i].imshow(images[i])\n",
    "    ax[i].axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WPofyIVOQib1"
   },
   "source": [
    "Observe que são imagens de conteúdo bem variado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jIUQLDcMQlw4"
   },
   "source": [
    "Para geração dos embeddings, nós vamos recorrer a um modelo chamado CLIP, da OpenAI. Vamos utilizar a API da biblioteca `transformers` para carregar este modelo. Também vamos carregar o pré-processador, que faz as normalizações necessárias nas imagens, para que elas sejam adequadamente comparáveis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kL0_b1qEXDUn"
   },
   "outputs": [],
   "source": [
    "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QlJn8xW9RHpt"
   },
   "source": [
    "Os inputs para o modelo são o resultado de passar as imagens pelo `processor`. Aqui, nós pedimos para `processor` retornar o resultado como tensores de PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sQnh7jYmRHRD"
   },
   "outputs": [],
   "source": [
    "inputs = processor(images=images, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rLObGOGkRa0Y"
   },
   "source": [
    "Para gerar os embeddings, nós passamos o conteúdo de `inputs` ao método `get_image_features` do modelo. Esta operação deve ser feita dentro do contexto `torch.no_grad()`, que indica ao modelo que ele não precisa armazenar os gradientes das operações, já que estamos usando o modelo em modo inferência, e não treinamento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KaL5FvdrRW8q"
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    embeddings = model.get_image_features(**inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nRTXqyYHRu2g"
   },
   "source": [
    "Agora, exiba o `shape` dos embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iv0-tRakgB09"
   },
   "outputs": [],
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xMbpVKmFR3D4"
   },
   "source": [
    "Observe que cada uma das 100 imagens é representada por um vetor com 512 dimensões."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1pcZE9sGR8te"
   },
   "source": [
    "Calcule a similaridade de coseno entre esses embeddings. Por conveniência, a função `cosine_similarity` do NumPy aceita tensores do PyTorch. Depois, exiba o `shape` da matriz obtida."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QUjYfHAFX3lF"
   },
   "outputs": [],
   "source": [
    "___\n",
    "___.___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VCwkL79FSGBZ"
   },
   "source": [
    "Esta matriz apresenta o índice de similaridade das 100 imagens entre si.\n",
    "\n",
    "Desta vez, nós estamos interessados em encontrar a imagem mais similar a cada uma das imagens desta pequena amostra. Como toda imagem é idêntica a ela mesma, nós precisamos primeiro zerar a diagonal principal da matriz de similaridade, que representa exatamente a similaridade da imagem a ela mesma."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_USmilJWgE6R"
   },
   "outputs": [],
   "source": [
    "np.fill_diagonal(similarity, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QLqdLMiySlKo"
   },
   "source": [
    "Agora, plote as 5 primeiras imagens do dataset, pareadas com a sua imagem mais similar. Organize as imagens em uma grid 2x5, com a linha superior sendo as 5 primeiras imagens, e a linha inferior exibindo a imagem similar. Para encontrar a imagem similar, nós usamos a função `np.argmax`, que retorna o índice correspondente ao valor máximo de um array. Este array, no caso, é a linha da matriz `similarity` correspondente à imagem original. Mãos à obra!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rfkRIDxNgP4G"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, 5, figsize=(20, 7))\n",
    "for i in range(___):\n",
    "    image = images[___]\n",
    "    i_max_sim = np.argmax(similarity[___])\n",
    "    similar_image = images[i_max_sim]\n",
    "    ___.imshow(image)\n",
    "    ___.axis('off')\n",
    "    ___.___(similar_image)\n",
    "    ___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WxNk9sD9TSzJ"
   },
   "source": [
    "Os resultados são de fato muito úteis! A imagem mais parecida com a primeira também contém ovelhas; a segunda também mostra um avião; a terceira também mostra uma pessoa; a quarta também mostra o mar e um barco; e a quinta também mostra comida. Impressionante, não?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "05eCBH5KTjAz"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MswgizCsTks6"
   },
   "source": [
    "Com isso nós terminamos este curso. Esperamos que você tenha aprendido a importância da Álgebra Linear para a área de Machine Learning, e que possa levar esse conhecimento para sua carreira.\n",
    "\n",
    "Até a próxima!"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [
    {
     "file_id": "1dACKDKTH4_tXfDQlWZS7yl18qyNULpb0",
     "timestamp": 1738678834458
    },
    {
     "file_id": "1P4uUtwzkpfdcw9d6suyEA59HI9pZr4el",
     "timestamp": 1730561198954
    }
   ],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
